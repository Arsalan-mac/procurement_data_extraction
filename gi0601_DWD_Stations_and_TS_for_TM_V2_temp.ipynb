{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DWD CDC Time Series, Merge with Station Description and Append \n",
    "\n",
    "The main idea behind this activity is to reformat and merge time series (here we use hourly precipitation) from the DWD Climate Data Center in such a way that it can be used with the **QGIS time manager extension**. \n",
    "\n",
    "This extension allows to filter an attribute table of a vector layer (e.g. points representing precipitation stations plus precipitation data) with a time stamp column. The extension limits the attribute table to the records matching the particular time stamp provided by the time manager extension (e.g. by the user moving the time slider). This selected subset of the attribute table is then used to change the sympology of the vector layer according to the variable of interest (e.g. precipitation rate).\n",
    "\n",
    "The QGIS time manager extension approach is a bit brute force, because each individual measurement at a station at a given time is one feature (row in the table), i.e. a time series at station X with hourly resolution for a day (24 values) entails 24 different features with the same station id and the corresponding coordinates but different times. As of now this 1:n relationship can only be realized by importing a CSV file with the according structure. \n",
    "\n",
    "(At least I was not able to generate the required view on a 1:n relationship by merging a point vector layer with precipitation station locations and an imported CSV time series table.)\n",
    "\n",
    "The final data format is a concatenation of time series together with geographic location in 2D (e.g. lat, lon). The required data format looks principly like this:\n",
    "\n",
    "\n",
    "| station_id |        name        |   lat   |   lon  |        meas_time       | prec_rate |\n",
    "|:----------:|:------------------:|:-------:|:------:|:----------------------:|:---------:|\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T08:00:00UTC |       1.5 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T09:00:00UTC |       1.7 |\n",
    "|       1595 | Gelsenkirchen-Buer | 51.5762 | 7.0652 | 2018-12-07T10:00:00UTC |       0.1 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T08:00:00UTC |       0.8 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T09:00:00UTC |       0.4 |\n",
    "|      13670 | Duisburg-Baerl     | 51.5088 | 6.7018 | 2018-12-07T10:00:00UTC |       0.0 |\n",
    "|        ... | ...                |     ... |    ... |                    ... |       ... |\n",
    "\n",
    "\n",
    "(Table generated with https://www.tablesgenerator.com/markdown_tables)\n",
    "\n",
    "To achieve this the precipitation time series (station_id, meas_time, prec_rate) have to be merged with the station metadata (station_id, lat, lon) coming from the a CSV file generated in an earlier activity. We use Pandas to read, join and append the data to generate the final CSV file to be imported as a point layer to QGIS. \n",
    "\n",
    "This final data format is far from being optimal because of large size and highly redundant information. This is a challenge for QGIS which loses responsiveness with large data. To jsut show the principle it is advisable to limit to size of the problem. \n",
    "\n",
    "The following filters (selection criteria) are applied:\n",
    "\n",
    "  * Precipitation stations in NRW only (approx. 127 stations) \n",
    "  * Hourly precipitation data\n",
    "  * Time interval from 2018-12-01 to last date in precipitation data set \n",
    "  \n",
    "Still: 40 days * 24 hrs / day * 127 stations = 121920 records leading to 121920 features in a point layer in QGIS. \n",
    "\n",
    "In fact, the resulting number of records is arround 91000. The reason might be that not all stations in the station list have time series. This has to be checked carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FTP Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"opendata.dwd.de\"\n",
    "user   = \"anonymous\"\n",
    "passwd = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Directory Definition and Station Description Filename Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic of interest.\n",
    "#topic_dir = \"/daily/more_precip/historical/\"\n",
    "topic_dir = \"/daily/soil_temperature/historical/\"\n",
    "\n",
    "# This is the search pattern common to ALL station description file names \n",
    "station_desc_pattern = \"_Beschreibung_Stationen.txt\"\n",
    "\n",
    "# Below this directory tree node all climate data are stored.\n",
    "ftp_climate_data_dir = \"/climate_environment/CDC/observations_germany/climate/\"\n",
    "ftp_dir =  ftp_climate_data_dir + topic_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ftp_dir         = \"data/original/DWD/\"      # Local directory to store local ftp data copies, the local data source or input data. \n",
    "local_ftp_station_dir = local_ftp_dir + topic_dir # Local directory where local station info is located\n",
    "local_ftp_ts_dir      = local_ftp_dir + topic_dir # Local directory where time series downloaded from ftp are located\n",
    "\n",
    "local_generated_dir   = \"data/generated/DWD/\" # The generated of derived data in contrast to local_ftp_dir\n",
    "local_station_dir     = local_generated_dir + topic_dir # Derived station data, i.e. the CSV file\n",
    "local_ts_merged_dir   = local_generated_dir + topic_dir # Parallel merged time series, wide data frame with one TS per column\n",
    "local_ts_appended_dir = local_generated_dir + topic_dir # Serially appended time series, long data frame for QGIS TimeManager Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/original/DWD/\n",
      "data/original/DWD//daily/soil_temperature/historical/\n",
      "data/original/DWD//daily/soil_temperature/historical/\n",
      "\n",
      "data/generated/DWD/\n",
      "data/generated/DWD//daily/soil_temperature/historical/\n",
      "data/generated/DWD//daily/soil_temperature/historical/\n",
      "data/generated/DWD//daily/soil_temperature/historical/\n"
     ]
    }
   ],
   "source": [
    "print(local_ftp_dir)\n",
    "print(local_ftp_station_dir)\n",
    "print(local_ftp_ts_dir)\n",
    "print()\n",
    "print(local_generated_dir)\n",
    "print(local_station_dir)\n",
    "print(local_ts_merged_dir)\n",
    "print(local_ts_appended_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(local_ftp_dir,exist_ok = True) # it does not complain if the dir already exists.\n",
    "os.makedirs(local_ftp_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ftp_ts_dir,exist_ok = True)\n",
    "\n",
    "os.makedirs(local_generated_dir,exist_ok = True)\n",
    "os.makedirs(local_station_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_merged_dir,exist_ok = True)\n",
    "os.makedirs(local_ts_appended_dir,exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230 Login successful.\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "ftp = ftplib.FTP(server)\n",
    "res = ftp.login(user=user, passwd = passwd)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = ftp.cwd(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftp.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FTP Grab File Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grabFile(ftpfullname,localfullname):\n",
    "    try:\n",
    "        ret = ftp.cwd(\".\") # A dummy action to chack the connection and to provoke an exception if necessary.\n",
    "        localfile = open(localfullname, 'wb')\n",
    "        ftp.retrbinary('RETR ' + ftpfullname, localfile.write, 1024)\n",
    "        localfile.close()\n",
    "    \n",
    "    except ftplib.error_perm:\n",
    "        print(\"FTP ERROR. Operation not permitted. File not found?\")\n",
    "\n",
    "    except ftplib.error_temp:\n",
    "        print(\"FTP ERROR. Timeout.\")\n",
    "\n",
    "    except ConnectionAbortedError:\n",
    "        print(\"FTP ERROR. Connection aborted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Pandas Dataframe from FTP Directory Listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def gen_df_from_ftp_dir_listing(ftp, ftpdir):\n",
    "    lines = []\n",
    "    flist = []\n",
    "    try:    \n",
    "        res = ftp.retrlines(\"LIST \"+ftpdir, lines.append)\n",
    "    except:\n",
    "        print(\"Error: ftp.retrlines() failed. ftp timeout? Reconnect!\")\n",
    "        return\n",
    "        \n",
    "    if len(lines) == 0:\n",
    "        print(\"Error: ftp dir is empty\")\n",
    "        return\n",
    "    \n",
    "    for line in lines:\n",
    "#        print(line)\n",
    "        [ftype, fsize, fname] = [line[0:1], int(line[31:42]), line[56:]]\n",
    "#        itemlist = [line[0:1], int(line[31:42]), line[56:]]\n",
    "#        flist.append(itemlist)\n",
    "        \n",
    "        fext = os.path.splitext(fname)[-1]\n",
    "        \n",
    "        if fext == \".zip\":\n",
    "            station_id = int(fname.split(\"_\")[2])\n",
    "        else:\n",
    "            station_id = -1 \n",
    "        \n",
    "        flist.append([station_id, fname, fext, fsize, ftype])\n",
    "        \n",
    "        \n",
    "\n",
    "    df_ftpdir = pd.DataFrame(flist,columns=[\"station_id\", \"name\", \"ext\", \"size\", \"type\"])\n",
    "    return(df_ftpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ftpdir = gen_df_from_ftp_dir_listing(ftp, ftp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>BESCHREIBUNG_obsgermany_climate_daily_soil_tem...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>69925</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>DESCRIPTION_obsgermany_climate_daily_soil_temp...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>69373</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>EB_Tageswerte_Beschreibung_Stationen.txt</td>\n",
       "      <td>.txt</td>\n",
       "      <td>99393</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>tageswerte_EB_00003_19510101_20110331_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>219229</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>tageswerte_EB_00044_19810101_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>140794</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   station_id                                               name   ext  \\\n",
       "0          -1  BESCHREIBUNG_obsgermany_climate_daily_soil_tem...  .pdf   \n",
       "1          -1  DESCRIPTION_obsgermany_climate_daily_soil_temp...  .pdf   \n",
       "2          -1           EB_Tageswerte_Beschreibung_Stationen.txt  .txt   \n",
       "3           3     tageswerte_EB_00003_19510101_20110331_hist.zip  .zip   \n",
       "4          44     tageswerte_EB_00044_19810101_20191231_hist.zip  .zip   \n",
       "\n",
       "     size type  \n",
       "0   69925    -  \n",
       "1   69373    -  \n",
       "2   99393    -  \n",
       "3  219229    -  \n",
       "4  140794    -  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ftpdir.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with TS Zip Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ext</th>\n",
       "      <th>size</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>tageswerte_EB_00003_19510101_20110331_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>219229</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>tageswerte_EB_00044_19810101_20191231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>140794</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>tageswerte_EB_00052_19760101_20011231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>99143</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>tageswerte_EB_00071_19880701_20031231_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>59893</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>tageswerte_EB_00072_19870101_19950531_hist.zip</td>\n",
       "      <td>.zip</td>\n",
       "      <td>35575</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      name   ext    size type\n",
       "station_id                                                                   \n",
       "3           tageswerte_EB_00003_19510101_20110331_hist.zip  .zip  219229    -\n",
       "44          tageswerte_EB_00044_19810101_20191231_hist.zip  .zip  140794    -\n",
       "52          tageswerte_EB_00052_19760101_20011231_hist.zip  .zip   99143    -\n",
       "71          tageswerte_EB_00071_19880701_20031231_hist.zip  .zip   59893    -\n",
       "72          tageswerte_EB_00072_19870101_19950531_hist.zip  .zip   35575    -"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_ftpdir[\"ext\"]==\".zip\"\n",
    "df_zips = df_ftpdir[df_ftpdir[\"ext\"]==\".zip\"]\n",
    "df_zips.set_index(\"station_id\", inplace = True)\n",
    "df_zips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Station Description File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB_Tageswerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "station_fname = df_ftpdir[df_ftpdir['name'].str.contains(station_desc_pattern)][\"name\"].values[0]\n",
    "print(station_fname)\n",
    "\n",
    "# ALternative\n",
    "#station_fname2 = df_ftpdir[df_ftpdir[\"name\"].str.match(\"^.*Beschreibung_Stationen.*txt$\")][\"name\"].values[0]\n",
    "#print(station_fname2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grabFile: \n",
      "From: /climate_environment/CDC/observations_germany/climate//daily/soil_temperature/historical/EB_Tageswerte_Beschreibung_Stationen.txt\n",
      "To:   data/original/DWD//daily/soil_temperature/historical/EB_Tageswerte_Beschreibung_Stationen.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"grabFile: \")\n",
    "print(\"From: \" + ftp_dir + station_fname)\n",
    "print(\"To:   \" + local_ftp_station_dir + station_fname)\n",
    "grabFile(ftp_dir + station_fname, local_ftp_station_dir + station_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract column names. They are in German (de)\n",
    "# We have to use codecs because of difficulties with character encoding (German Umlaute)\n",
    "import codecs\n",
    "\n",
    "def station_desc_txt_to_csv(txtfile, csvfile):\n",
    "    file = codecs.open(txtfile,\"r\",\"utf-8\")\n",
    "    r = file.readline()\n",
    "    file.close()\n",
    "    colnames_de = r.split()\n",
    "    colnames_de\n",
    "    \n",
    "    translate = \\\n",
    "    {'Stations_id':'station_id',\n",
    "     'von_datum':'date_from',\n",
    "     'bis_datum':'date_to',\n",
    "     'Stationshoehe':'altitude',\n",
    "     'geoBreite': 'latitude',\n",
    "     'geoLaenge': 'longitude',\n",
    "     'Stationsname':'name',\n",
    "     'Bundesland':'state'}\n",
    "    \n",
    "    colnames_en = [translate[h] for h in colnames_de]\n",
    "    \n",
    "    # Skip the first two rows and set the column names.\n",
    "    df = pd.read_fwf(txtfile,skiprows=2,names=colnames_en, parse_dates=[\"date_from\",\"date_to\"],index_col = 0)\n",
    "    \n",
    "    # write csv\n",
    "    df.to_csv(csvfile, sep = \";\")\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>202</td>\n",
       "      <td>50.7827</td>\n",
       "      <td>6.0941</td>\n",
       "      <td>Aachen</td>\n",
       "      <td>Nordrhein-Westfalen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>44</td>\n",
       "      <td>52.9336</td>\n",
       "      <td>8.2370</td>\n",
       "      <td>Großenkneten</td>\n",
       "      <td>Niedersachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1976-01-01</td>\n",
       "      <td>2001-12-31</td>\n",
       "      <td>46</td>\n",
       "      <td>53.6623</td>\n",
       "      <td>10.1990</td>\n",
       "      <td>Ahrensburg-Wulfsdorf</td>\n",
       "      <td>Schleswig-Holstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1988-07-01</td>\n",
       "      <td>2003-12-31</td>\n",
       "      <td>759</td>\n",
       "      <td>48.2156</td>\n",
       "      <td>8.9784</td>\n",
       "      <td>Albstadt-Badkap</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>1995-05-31</td>\n",
       "      <td>794</td>\n",
       "      <td>48.2766</td>\n",
       "      <td>9.0001</td>\n",
       "      <td>Albstadt-Onstmettingen</td>\n",
       "      <td>Baden-Württemberg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "3          1951-01-01 2011-03-31       202   50.7827     6.0941   \n",
       "44         1981-01-01 2020-06-29        44   52.9336     8.2370   \n",
       "52         1976-01-01 2001-12-31        46   53.6623    10.1990   \n",
       "71         1988-07-01 2003-12-31       759   48.2156     8.9784   \n",
       "72         1987-01-01 1995-05-31       794   48.2766     9.0001   \n",
       "\n",
       "                              name                state  \n",
       "station_id                                               \n",
       "3                           Aachen  Nordrhein-Westfalen  \n",
       "44                    Großenkneten        Niedersachsen  \n",
       "52            Ahrensburg-Wulfsdorf   Schleswig-Holstein  \n",
       "71                 Albstadt-Badkap    Baden-Württemberg  \n",
       "72          Albstadt-Onstmettingen    Baden-Württemberg  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basename = os.path.splitext(station_fname)[0]\n",
    "df_stations = station_desc_txt_to_csv(local_ftp_station_dir + station_fname, local_station_dir + basename + \".csv\")\n",
    "df_stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Stations Located in NRW from Station Description Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  125,   154,   221,   232,   282,   320,   361,   502,   685,\n",
       "              856,   867,  1107,  1161,  1262,  1292,  1473,  1550,  1587,\n",
       "             2023,  2261,  2290,  2360,  2410,  2488,  2521,  2542,  2559,\n",
       "             2597,  2691,  2700,  2750,  2773,  2783,  2829,  2831,  2905,\n",
       "             3056,  3139,  3271,  3366,  3379,  3390,  3484,  3485,  3565,\n",
       "             3571,  3621,  3668,  3722,  3730,  3875,  3879,  3975,  4104,\n",
       "             4280,  4287,  4354,  4438,  4559,  4592,  4706,  4911,  5032,\n",
       "             5185,  5397,  5404,  5434,  5440,  5538,  5654,  5703,  5705,\n",
       "             5800,  5802,  5856,  5904,  6158,  6219,  6312,  6336,  7075,\n",
       "             7319,  7369,  7370,  7394,  7395,  7412,  7424, 15555],\n",
       "           dtype='int64', name='station_id')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isOperational = df_stations.date_to.max() \n",
    "\n",
    "station_ids_selected = df_stations[df_stations['state'].str.contains(\"Bayern\")].index \n",
    "station_ids_selected\n",
    "#isOperational\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>461</td>\n",
       "      <td>48.4254</td>\n",
       "      <td>10.9420</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>240</td>\n",
       "      <td>49.8743</td>\n",
       "      <td>10.9206</td>\n",
       "      <td>Bamberg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>1953-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>344</td>\n",
       "      <td>50.3066</td>\n",
       "      <td>10.9679</td>\n",
       "      <td>Lautertal-Oberlauter</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2261</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>565</td>\n",
       "      <td>50.3123</td>\n",
       "      <td>11.8760</td>\n",
       "      <td>Hof</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2597</td>\n",
       "      <td>1949-09-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>282</td>\n",
       "      <td>50.2240</td>\n",
       "      <td>10.0792</td>\n",
       "      <td>Kissingen, Bad</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3366</td>\n",
       "      <td>1953-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>406</td>\n",
       "      <td>48.2790</td>\n",
       "      <td>12.5024</td>\n",
       "      <td>Mühldorf</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3730</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>806</td>\n",
       "      <td>47.3984</td>\n",
       "      <td>10.2759</td>\n",
       "      <td>Oberstdorf</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5397</td>\n",
       "      <td>1972-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>440</td>\n",
       "      <td>49.6663</td>\n",
       "      <td>12.1845</td>\n",
       "      <td>Weiden</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5404</td>\n",
       "      <td>1972-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>477</td>\n",
       "      <td>48.4024</td>\n",
       "      <td>11.6946</td>\n",
       "      <td>Weihenstephan-Dürnast</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>1961-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>439</td>\n",
       "      <td>49.0115</td>\n",
       "      <td>10.9308</td>\n",
       "      <td>Weißenburg-Emetzheim</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5705</td>\n",
       "      <td>1977-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>268</td>\n",
       "      <td>49.7704</td>\n",
       "      <td>9.9576</td>\n",
       "      <td>Würzburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_from    date_to  altitude  latitude  longitude  \\\n",
       "station_id                                                        \n",
       "232        1951-01-01 2020-06-29       461   48.4254    10.9420   \n",
       "282        1951-01-01 2020-06-29       240   49.8743    10.9206   \n",
       "867        1953-01-01 2020-06-29       344   50.3066    10.9679   \n",
       "2261       1951-01-01 2020-06-29       565   50.3123    11.8760   \n",
       "2597       1949-09-01 2020-06-29       282   50.2240    10.0792   \n",
       "3366       1953-01-01 2020-06-29       406   48.2790    12.5024   \n",
       "3730       1961-01-01 2020-06-29       806   47.3984    10.2759   \n",
       "5397       1972-01-01 2020-06-29       440   49.6663    12.1845   \n",
       "5404       1972-01-01 2020-06-29       477   48.4024    11.6946   \n",
       "5440       1961-01-01 2020-06-29       439   49.0115    10.9308   \n",
       "5705       1977-01-01 2020-06-29       268   49.7704     9.9576   \n",
       "\n",
       "                             name   state  \n",
       "station_id                                 \n",
       "232                      Augsburg  Bayern  \n",
       "282                       Bamberg  Bayern  \n",
       "867          Lautertal-Oberlauter  Bayern  \n",
       "2261                          Hof  Bayern  \n",
       "2597               Kissingen, Bad  Bayern  \n",
       "3366                     Mühldorf  Bayern  \n",
       "3730                   Oberstdorf  Bayern  \n",
       "5397                       Weiden  Bayern  \n",
       "5404        Weihenstephan-Dürnast  Bayern  \n",
       "5440         Weißenburg-Emetzheim  Bayern  \n",
       "5705                     Würzburg  Bayern  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create variable with TRUE if state is Nordrhein-Westfalen\n",
    "isNRW = df_stations['state'] == \"Bayern\"\n",
    "\n",
    "# Create variable with TRUE if date_to is latest date (indicates operation up to now)\n",
    "isOperational = df_stations['date_to'] == df_stations.date_to.max() \n",
    "\n",
    "isBefore1950 = df_stations['date_from'] < '1980'\n",
    "\n",
    "# select on both conditions\n",
    "dfNRW = df_stations[isNRW & isOperational & isBefore1950]\n",
    "#print(\"Number of stations in NRW: \\n\", dfNRW.count())\n",
    "dfNRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      name   ext    size type\n",
      "station_id                                                                   \n",
      "3           tageswerte_EB_00003_19510101_20110331_hist.zip  .zip  219229    -\n",
      "44          tageswerte_EB_00044_19810101_20191231_hist.zip  .zip  140794    -\n",
      "52          tageswerte_EB_00052_19760101_20011231_hist.zip  .zip   99143    -\n",
      "71          tageswerte_EB_00071_19880701_20031231_hist.zip  .zip   59893    -\n",
      "72          tageswerte_EB_00072_19870101_19950531_hist.zip  .zip   35575    -\n",
      "...                                                    ...   ...     ...  ...\n",
      "14311       tageswerte_EB_14311_19870101_20051231_hist.zip  .zip   55798    -\n",
      "15000       tageswerte_EB_15000_20110401_20191231_hist.zip  .zip   33708    -\n",
      "15207       tageswerte_EB_15207_20131101_20191231_hist.zip  .zip   25379    -\n",
      "15444       tageswerte_EB_15444_20140901_20191231_hist.zip  .zip   22584    -\n",
      "15555       tageswerte_EB_15555_20160501_20191231_hist.zip  .zip   16905    -\n",
      "\n",
      "[483 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_zips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download TS Data from FTP Server\n",
    "\n",
    "Problem: Not all stations listed in the station description file are associated with a time series (zip file)! The stations in the description file and the set of stations whoch are TS data provided for (zip files) do not match perfectly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[232, 282, 867, 2261, 2597, 3366, 3730, 5397, 5404, 5440, 5705]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dfNRW.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tageswerte_EB_00232_19510101_20191231_hist.zip\n",
      "tageswerte_EB_00282_19510101_20191231_hist.zip\n",
      "tageswerte_EB_00867_19530101_20191231_hist.zip\n",
      "tageswerte_EB_02261_19510101_20191231_hist.zip\n",
      "tageswerte_EB_02597_19490901_20191231_hist.zip\n",
      "tageswerte_EB_03366_19530101_20191231_hist.zip\n",
      "tageswerte_EB_03730_19610101_20191231_hist.zip\n",
      "tageswerte_EB_05397_19720101_20191231_hist.zip\n",
      "tageswerte_EB_05404_19720101_20191231_hist.zip\n",
      "tageswerte_EB_05440_19610101_20191231_hist.zip\n",
      "tageswerte_EB_05705_19770101_20191231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "# Add the names of the zip files only to a list. \n",
    "local_zip_list = []\n",
    "\n",
    "station_ids_selected = list(dfNRW.index)\n",
    "\n",
    "for station_id in station_ids_selected:\n",
    "    try:\n",
    "        fname = df_zips[\"name\"][station_id]\n",
    "        print(fname)\n",
    "        grabFile(ftp_dir + fname, local_ftp_ts_dir + fname)\n",
    "        local_zip_list.append(fname)\n",
    "    except:\n",
    "        print(\"WARNING: TS file for key %d not found in FTP directory.\" % station_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join (Merge) the Time Series Columns\n",
    "\n",
    "https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_ts_to_df(fname):\n",
    "    \n",
    "    dateparse = lambda dates: [pd.datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "\n",
    "    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "\n",
    "    #df = pd.read_csv(fname, delimiter=\";\", encoding=\"iso8859_2\",\\\n",
    "    #             index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse)\n",
    "    \n",
    "    # https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n",
    "\n",
    "    # Column headers: remove leading blanks (strip), replace \" \" with \"_\", and convert to lower case.\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def temp_ts_to_df(fname):\n",
    "    \n",
    "#    dateparse = lambda dates: [pd.datetime.strptime(str(d), '%Y%m%d') for d in dates]\n",
    "\n",
    "#    df = pd.read_csv(fname, delimiter=\";\", encoding=\"utf8\", index_col=\"MESS_DATUM_BEGINN\", parse_dates = [\"MESS_DATUM_BEGINN\"], date_parser = dateparse, na_values = [-999.0, -999])\n",
    "\n",
    "    #df = pd.read_csv(fname, delimiter=\";\", encoding=\"iso8859_2\",\\\n",
    "    #             index_col=\"MESS_DATUM\", parse_dates = [\"MESS_DATUM\"], date_parser = dateparse)\n",
    "    \n",
    "    # https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd\n",
    "\n",
    "    # Column headers: remove leading blanks (strip), replace \" \" with \"_\", and convert to lower case.\n",
    "#    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "#    df.index.name = df.index.name.strip().lower().replace(' ', '_').replace('(', '').replace(')', '')\n",
    "#    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_merge():\n",
    "    # Very compact code.\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = temp_ts_to_df(myfile)\n",
    "#                s = dftmp[\"r1\"].rename(dftmp[\"stations_id\"][0]).to_frame()\n",
    "                s = dftmp[\"stations_id\"].to_frame()\n",
    "                # outer merge.\n",
    "                df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "    df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prec_ts_merge():\n",
    "    # Very compact code.\n",
    "#    df = pd.DataFrame()\n",
    "#    for elt in local_zip_list:\n",
    "#        ffname = local_ftp_ts_dir + elt\n",
    "#        print(\"Zip archive: \" + ffname)\n",
    "#        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "#            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "#            print(\"Extract product file: %s\" % prodfilename)\n",
    "#            print()\n",
    "#            with myzip.open(prodfilename) as myfile:\n",
    "#                dftmp = temp_ts_to_df(myfile)\n",
    "#                s = dftmp[\"ja_tt\"].rename(dftmp[\"stations_id\"][0]).to_frame()\n",
    "                # outer merge.\n",
    "#                df = pd.merge(df, s, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "#    df.index.rename(name = \"time\", inplace = True)\n",
    "#    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00232_19510101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20191231_00232.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00282_19510101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20191231_00282.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00867_19530101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19530101_20191231_00867.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02261_19510101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20191231_02261.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02597_19490901_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19490901_20191231_02597.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03366_19530101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19530101_20191231_03366.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03730_19610101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19610101_20191231_03730.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05397_19720101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19720101_20191231_05397.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05404_19720101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19720101_20191231_05404.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05440_19610101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19610101_20191231_05440.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05705_19770101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19770101_20191231_05705.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merged_ts = ts_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id_x</th>\n",
       "      <th>stations_id_y</th>\n",
       "      <th>stations_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1949-09-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949-09-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949-09-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949-09-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1949-09-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stations_id_x  stations_id_y  stations_id_x  stations_id_y  \\\n",
       "time                                                                     \n",
       "1949-09-01            NaN            NaN            NaN            NaN   \n",
       "1949-09-02            NaN            NaN            NaN            NaN   \n",
       "1949-09-03            NaN            NaN            NaN            NaN   \n",
       "1949-09-04            NaN            NaN            NaN            NaN   \n",
       "1949-09-05            NaN            NaN            NaN            NaN   \n",
       "\n",
       "            stations_id_x  stations_id_y  stations_id_x  stations_id_y  \\\n",
       "time                                                                     \n",
       "1949-09-01         2597.0            NaN            NaN            NaN   \n",
       "1949-09-02         2597.0            NaN            NaN            NaN   \n",
       "1949-09-03         2597.0            NaN            NaN            NaN   \n",
       "1949-09-04         2597.0            NaN            NaN            NaN   \n",
       "1949-09-05         2597.0            NaN            NaN            NaN   \n",
       "\n",
       "            stations_id_x  stations_id_y  stations_id  \n",
       "time                                                   \n",
       "1949-09-01            NaN            NaN          NaN  \n",
       "1949-09-02            NaN            NaN          NaN  \n",
       "1949-09-03            NaN            NaN          NaN  \n",
       "1949-09-04            NaN            NaN          NaN  \n",
       "1949-09-05            NaN            NaN          NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = plt.figure(dpi= 136, figsize=(8,4))\n",
    "#ax1 = fig.add_subplot(221)\n",
    "#ax2 = fig.add_subplot(222)\n",
    "#ax3 = fig.add_subplot(223)\n",
    "#ax4 = fig.add_subplot(224)\n",
    "#df_merged_ts[1024].plot(ax = ax1)\n",
    "#df_merged_ts[1232].plot(ax = ax1)\n",
    "#df_merged_ts[1246].plot(ax = ax2)\n",
    "#df_merged_ts[1277].plot(ax = ax3)\n",
    "#df_merged_ts[1300].plot(ax = ax4)\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "# plot\n",
    "#sns.set_style('ticks')\n",
    "#fig1, ax1 = plt.subplots(dpi = 400, figsize = (12,24))\n",
    "\n",
    "#sns.heatmap(df_merged_ts, cmap='RdYlGn_r', annot=False, ax = ax1)\n",
    "#sns.heatmap(df_merged_ts, cmap='coolwarm', annot=True, vmin = 8, vmax = 12, ax = ax1)\n",
    "\n",
    "# _r reverses the normal order of the color map 'RdYlGn'\n",
    "\n",
    "#sns.heatmap(df, cmap='coolwarm', annot=True, vmin = 8, vmax = 12, ax = ax)\n",
    "#ax1.set_yticklabels(df_merged_ts.index.strftime('%Y'))\n",
    "#plt.show()\n",
    "#fig1.savefig('example1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts.to_csv(local_ts_merged_dir + \"ts_merged_temp.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed = df_merged_ts.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed.index.names = ['station_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>time</th>\n",
       "      <th>1949-09-01</th>\n",
       "      <th>1949-09-02</th>\n",
       "      <th>1949-09-03</th>\n",
       "      <th>1949-09-04</th>\n",
       "      <th>1949-09-05</th>\n",
       "      <th>1949-09-06</th>\n",
       "      <th>1949-09-07</th>\n",
       "      <th>1949-09-08</th>\n",
       "      <th>1949-09-09</th>\n",
       "      <th>1949-09-10</th>\n",
       "      <th>...</th>\n",
       "      <th>2019-12-22</th>\n",
       "      <th>2019-12-23</th>\n",
       "      <th>2019-12-24</th>\n",
       "      <th>2019-12-25</th>\n",
       "      <th>2019-12-26</th>\n",
       "      <th>2019-12-27</th>\n",
       "      <th>2019-12-28</th>\n",
       "      <th>2019-12-29</th>\n",
       "      <th>2019-12-30</th>\n",
       "      <th>2019-12-31</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>stations_id_x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stations_id_y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stations_id_x</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>867.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stations_id_y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>2261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>stations_id_x</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "      <td>2597.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25682 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "time           1949-09-01  1949-09-02  1949-09-03  1949-09-04  1949-09-05  \\\n",
       "station_id                                                                  \n",
       "stations_id_x         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_y         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_x         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_y         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_x      2597.0      2597.0      2597.0      2597.0      2597.0   \n",
       "\n",
       "time           1949-09-06  1949-09-07  1949-09-08  1949-09-09  1949-09-10  \\\n",
       "station_id                                                                  \n",
       "stations_id_x         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_y         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_x         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_y         NaN         NaN         NaN         NaN         NaN   \n",
       "stations_id_x      2597.0      2597.0      2597.0      2597.0      2597.0   \n",
       "\n",
       "time           ...  2019-12-22  2019-12-23  2019-12-24  2019-12-25  \\\n",
       "station_id     ...                                                   \n",
       "stations_id_x  ...       232.0       232.0       232.0       232.0   \n",
       "stations_id_y  ...       282.0       282.0       282.0       282.0   \n",
       "stations_id_x  ...       867.0       867.0       867.0       867.0   \n",
       "stations_id_y  ...      2261.0      2261.0      2261.0      2261.0   \n",
       "stations_id_x  ...      2597.0      2597.0      2597.0      2597.0   \n",
       "\n",
       "time           2019-12-26  2019-12-27  2019-12-28  2019-12-29  2019-12-30  \\\n",
       "station_id                                                                  \n",
       "stations_id_x       232.0       232.0       232.0       232.0       232.0   \n",
       "stations_id_y       282.0       282.0       282.0       282.0       282.0   \n",
       "stations_id_x       867.0       867.0       867.0       867.0       867.0   \n",
       "stations_id_y      2261.0      2261.0      2261.0      2261.0      2261.0   \n",
       "stations_id_x      2597.0      2597.0      2597.0      2597.0      2597.0   \n",
       "\n",
       "time           2019-12-31  \n",
       "station_id                 \n",
       "stations_id_x       232.0  \n",
       "stations_id_y       282.0  \n",
       "stations_id_x       867.0  \n",
       "stations_id_y      2261.0  \n",
       "stations_id_x      2597.0  \n",
       "\n",
       "[5 rows x 25682 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_ts_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_ts_transposed.to_csv(local_ts_merged_dir + \"ts_merged_transposed.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_append():\n",
    "    # Very compact code.\n",
    "    df = pd.DataFrame()\n",
    "    for elt in local_zip_list:\n",
    "        ffname = local_ftp_ts_dir + elt\n",
    "        print(\"Zip archive: \" + ffname)\n",
    "        with ZipFile(ffname) as myzip:\n",
    "            # read the time series data from the file starting with \"produkt\"\n",
    "            prodfilename = [elt for elt in myzip.namelist() if elt.split(\"_\")[0]==\"produkt\"][0] \n",
    "            print(\"Extract product file: %s\" % prodfilename)\n",
    "            print()\n",
    "            with myzip.open(prodfilename) as myfile:\n",
    "                dftmp = temp_ts_to_df(myfile)\n",
    "                dftmp=dftmp.loc[(dftmp.index>='2016-06-01 00:00:00')&(dftmp.index<='2020-06-01 00:00:00')]\n",
    "                dftmp = dftmp.merge(df_stations,how=\"inner\",left_on=\"stations_id\",right_on=\"station_id\",right_index=True)\n",
    "#                print(dftmp.head(5))\n",
    "                df = df.append(dftmp)\n",
    "\n",
    "    #df.index.names = [\"year\"]\n",
    "    #df.index.rename(name = \"time\", inplace = True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00232_19510101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20191231_00232.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00282_19510101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20191231_00282.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_00867_19530101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19530101_20191231_00867.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02261_19510101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19510101_20191231_02261.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_02597_19490901_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19490901_20191231_02597.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03366_19530101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19530101_20191231_03366.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_03730_19610101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19610101_20191231_03730.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05397_19720101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19720101_20191231_05397.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05404_19720101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19720101_20191231_05404.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05440_19610101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19610101_20191231_05440.txt\n",
      "\n",
      "Zip archive: data/original/DWD//daily/soil_temperature/historical/tageswerte_EB_05705_19770101_20191231_hist.zip\n",
      "Extract product file: produkt_erdbo_tag_19770101_20191231_05705.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_appended_ts = ts_append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stations_id</th>\n",
       "      <th>qn_2</th>\n",
       "      <th>v_te002m</th>\n",
       "      <th>v_te005m</th>\n",
       "      <th>v_te010m</th>\n",
       "      <th>v_te020m</th>\n",
       "      <th>v_te050m</th>\n",
       "      <th>eor</th>\n",
       "      <th>date_from</th>\n",
       "      <th>date_to</th>\n",
       "      <th>altitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mess_datum</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2016-06-01</td>\n",
       "      <td>232</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.9</td>\n",
       "      <td>17.6</td>\n",
       "      <td>17.1</td>\n",
       "      <td>14.9</td>\n",
       "      <td>eor</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>461</td>\n",
       "      <td>48.4254</td>\n",
       "      <td>10.942</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>232</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.7</td>\n",
       "      <td>16.8</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.9</td>\n",
       "      <td>eor</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>461</td>\n",
       "      <td>48.4254</td>\n",
       "      <td>10.942</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-06-03</td>\n",
       "      <td>232</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.8</td>\n",
       "      <td>eor</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>461</td>\n",
       "      <td>48.4254</td>\n",
       "      <td>10.942</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-06-04</td>\n",
       "      <td>232</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17.6</td>\n",
       "      <td>14.8</td>\n",
       "      <td>eor</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>461</td>\n",
       "      <td>48.4254</td>\n",
       "      <td>10.942</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2016-06-05</td>\n",
       "      <td>232</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.6</td>\n",
       "      <td>18.3</td>\n",
       "      <td>17.8</td>\n",
       "      <td>15.0</td>\n",
       "      <td>eor</td>\n",
       "      <td>1951-01-01</td>\n",
       "      <td>2020-06-29</td>\n",
       "      <td>461</td>\n",
       "      <td>48.4254</td>\n",
       "      <td>10.942</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Bayern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stations_id  qn_2  v_te002m  v_te005m  v_te010m  v_te020m  \\\n",
       "mess_datum                                                              \n",
       "2016-06-01          232     3       NaN      17.9      17.6      17.1   \n",
       "2016-06-02          232     3       NaN      16.7      16.8      16.8   \n",
       "2016-06-03          232     3       NaN      17.9      17.5      16.8   \n",
       "2016-06-04          232     3       NaN      18.9      18.4      17.6   \n",
       "2016-06-05          232     3       NaN      18.6      18.3      17.8   \n",
       "\n",
       "            v_te050m  eor  date_from    date_to  altitude  latitude  \\\n",
       "mess_datum                                                            \n",
       "2016-06-01      14.9  eor 1951-01-01 2020-06-29       461   48.4254   \n",
       "2016-06-02      14.9  eor 1951-01-01 2020-06-29       461   48.4254   \n",
       "2016-06-03      14.8  eor 1951-01-01 2020-06-29       461   48.4254   \n",
       "2016-06-04      14.8  eor 1951-01-01 2020-06-29       461   48.4254   \n",
       "2016-06-05      15.0  eor 1951-01-01 2020-06-29       461   48.4254   \n",
       "\n",
       "            longitude      name   state  \n",
       "mess_datum                               \n",
       "2016-06-01     10.942  Augsburg  Bayern  \n",
       "2016-06-02     10.942  Augsburg  Bayern  \n",
       "2016-06-03     10.942  Augsburg  Bayern  \n",
       "2016-06-04     10.942  Augsburg  Bayern  \n",
       "2016-06-05     10.942  Augsburg  Bayern  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_appended_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appended_ts.to_csv(local_ts_appended_dir + \"ts_appended_temp.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
